{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fc85dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/root/miniconda3/envs/sp/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8627fe3b-fefe-4e95-bcac-86b0c138a51e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.4.1 in central\n",
      ":: resolution report :: resolve 75ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.4.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8627fe3b-fefe-4e95-bcac-86b0c138a51e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 02:16:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/19 02:16:29 WARN DependencyUtils: Local jar /data/jupyter-data/lab08/kafka-clients-3.4.0.jar does not exist, skipping.\n",
      "23/05/19 02:16:29 WARN DependencyUtils: Local jar /data/jupyter-data/lab08/spark-sql-kafka-0-10_2.12-3.3.1.jar does not exist, skipping.\n",
      "23/05/19 02:16:29 WARN DependencyUtils: Local jar /data/jupyter-data/lab08/spark-token-provider-kafka-0-10_2.12-3.3.1.jar does not exist, skipping.\n",
      "23/05/19 02:16:29 WARN DependencyUtils: Local jar /data/jupyter-data/lab08/commons-pool2-2.11.1.jar does not exist, skipping.\n",
      "23/05/19 02:16:29 INFO SparkContext: Running Spark version 3.3.1\n",
      "23/05/19 02:16:29 INFO ResourceUtils: ==============================================================\n",
      "23/05/19 02:16:29 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/05/19 02:16:29 INFO ResourceUtils: ==============================================================\n",
      "23/05/19 02:16:29 INFO SparkContext: Submitted application: Pro2\n",
      "23/05/19 02:16:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/05/19 02:16:29 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/05/19 02:16:29 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/05/19 02:16:29 INFO SecurityManager: Changing view acls to: root\n",
      "23/05/19 02:16:29 INFO SecurityManager: Changing modify acls to: root\n",
      "23/05/19 02:16:29 INFO SecurityManager: Changing view acls groups to: \n",
      "23/05/19 02:16:29 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/05/19 02:16:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "23/05/19 02:16:29 INFO Utils: Successfully started service 'sparkDriver' on port 38761.\n",
      "23/05/19 02:16:29 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/05/19 02:16:29 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/05/19 02:16:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/05/19 02:16:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/05/19 02:16:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/05/19 02:16:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ccdc8242-cc08-48a8-9485-4d8e6c42c568\n",
      "23/05/19 02:16:29 INFO MemoryStore: MemoryStore started with capacity 2004.6 MiB\n",
      "23/05/19 02:16:29 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/05/19 02:16:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/19 02:16:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/05/19 02:16:29 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "23/05/19 02:16:29 ERROR SparkContext: Failed to add /data/jupyter-data/lab08/kafka-clients-3.4.0.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /data/jupyter-data/lab08/kafka-clients-3.4.0.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/05/19 02:16:29 ERROR SparkContext: Failed to add /data/jupyter-data/lab08/spark-sql-kafka-0-10_2.12-3.3.1.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /data/jupyter-data/lab08/spark-sql-kafka-0-10_2.12-3.3.1.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/05/19 02:16:29 ERROR SparkContext: Failed to add /data/jupyter-data/lab08/spark-token-provider-kafka-0-10_2.12-3.3.1.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /data/jupyter-data/lab08/spark-token-provider-kafka-0-10_2.12-3.3.1.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/05/19 02:16:29 ERROR SparkContext: Failed to add /data/jupyter-data/lab08/commons-pool2-2.11.1.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /data/jupyter-data/lab08/commons-pool2-2.11.1.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/05/19 02:16:29 INFO SparkContext: Added file file:///root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.1.jar at file:///root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.1.jar with timestamp 1684462589167\n",
      "23/05/19 02:16:29 INFO Utils: Copying /root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.1.jar to /tmp/spark-d353a165-1a56-4bbb-9b44-fa7e858af498/userFiles-22b8445d-c33c-4038-9a71-d0ab8e6003a7/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.1.jar\n",
      "23/05/19 02:16:30 INFO Executor: Starting executor ID driver on host d313366b52a2\n",
      "23/05/19 02:16:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/05/19 02:16:30 INFO Executor: Fetching file:///root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.1.jar with timestamp 1684462589167\n",
      "23/05/19 02:16:30 INFO Utils: /root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.1.jar has been previously copied to /tmp/spark-d353a165-1a56-4bbb-9b44-fa7e858af498/userFiles-22b8445d-c33c-4038-9a71-d0ab8e6003a7/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.1.jar\n",
      "23/05/19 02:16:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40149.\n",
      "23/05/19 02:16:30 INFO NettyBlockTransferService: Server created on d313366b52a2:40149\n",
      "23/05/19 02:16:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/05/19 02:16:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, d313366b52a2, 40149, None)\n",
      "23/05/19 02:16:30 INFO BlockManagerMasterEndpoint: Registering block manager d313366b52a2:40149 with 2004.6 MiB RAM, BlockManagerId(driver, d313366b52a2, 40149, None)\n",
      "23/05/19 02:16:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, d313366b52a2, 40149, None)\n",
      "23/05/19 02:16:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, d313366b52a2, 40149, None)\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.pretrained import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pyspark.context import SparkContext \n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import os \n",
    "os.environ['PYSPARK_PYTHON'] = \"/root/miniconda3/envs/sp/bin/python\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\", \"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000m\")\\\n",
    "    .config(\"spark.jsl.settings.pretrained.cache_folder\", \"sample_data/pretrained\")\\\n",
    "    .config(\"spark.jsl.settings.storage.cluster_tmp_dir\", \"sample_dataorage\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.1\")\\\n",
    "    .config(\n",
    "    \"spark.jars\",\n",
    "    \"/data/jupyter-data/lab08/kafka-clients-3.4.0.jar,/data/jupyter-data/lab08/spark-sql-kafka-0-10_2.12-3.3.1.jar, \\\n",
    "    /data/jupyter-data/lab08/spark-token-provider-kafka-0-10_2.12-3.3.1.jar, \\\n",
    "    /data/jupyter-data/lab08/commons-pool2-2.11.1.jar\") \\\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\",\"/root/miniconda3/3/envs/sp/bin/python\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .appName(\"Pro2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ba0518",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.com.johnsnowlabs.nlp.annotators.classifier.dl.DistilBertForQuestionAnswering.\n: java.lang.NoClassDefFoundError: com/amazonaws/AmazonServiceException\n\tat com.johnsnowlabs.nlp.serialization.Feature.<init>(Feature.scala:36)\n\tat com.johnsnowlabs.nlp.serialization.MapFeature.<init>(Feature.scala:195)\n\tat com.johnsnowlabs.nlp.annotators.classifier.dl.DistilBertForQuestionAnswering.<init>(DistilBertForQuestionAnswering.scala:149)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonServiceException\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-cased-distilled-squad\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m document_assembler \u001b[38;5;241m=\u001b[39m MultiDocumentAssembler() \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue_ans\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39msetOutputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_question\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_context\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_ans\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m spanClassifier_loaded \u001b[38;5;241m=\u001b[39m \u001b[43mDistilBertForQuestionAnswering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_spark_nlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_question\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument_context\u001b[39m\u001b[38;5;124m'\u001b[39m])\\\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline()\u001b[38;5;241m.\u001b[39msetStages([\n\u001b[1;32m     12\u001b[0m     document_assembler,\n\u001b[1;32m     13\u001b[0m     spanClassifier_loaded\n\u001b[1;32m     14\u001b[0m ])\n",
      "File \u001b[0;32m~/miniconda3/envs/sp/lib/python3.10/site-packages/pyspark/ml/util.py:353\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[0;32m~/miniconda3/envs/sp/lib/python3.10/site-packages/sparknlp/internal/annotator_java_ml.py:23\u001b[0m, in \u001b[0;36mAnnotatorJavaMLReadable.read\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns an MLReader instance for this class.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AnnotatorJavaMLReader(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sp/lib/python3.10/site-packages/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sp/lib/python3.10/site-packages/sparknlp/annotator/classifier_dl/distil_bert_for_question_answering.py:131\u001b[0m, in \u001b[0;36mDistilBertForQuestionAnswering.__init__\u001b[0;34m(self, classname, java_model)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, classname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.johnsnowlabs.nlp.annotators.classifier.dl.DistilBertForQuestionAnswering\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    130\u001b[0m              java_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDistilBertForQuestionAnswering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclassname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjava_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjava_model\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(\n\u001b[1;32m    136\u001b[0m         batchSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m    137\u001b[0m         maxSentenceLength\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m    138\u001b[0m         caseSensitive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/sp/lib/python3.10/site-packages/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sp/lib/python3.10/site-packages/sparknlp/common/annotator_model.py:37\u001b[0m, in \u001b[0;36mAnnotatorModel.__init__\u001b[0;34m(self, classname, java_model)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classname \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m java_model:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_java_class_name \u001b[38;5;241m=\u001b[39m classname\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m java_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_from_java()\n",
      "File \u001b[0;32m~/miniconda3/envs/sp/lib/python3.10/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjava_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sp/lib/python3.10/site-packages/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/envs/sp/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/envs/sp/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.com.johnsnowlabs.nlp.annotators.classifier.dl.DistilBertForQuestionAnswering.\n: java.lang.NoClassDefFoundError: com/amazonaws/AmazonServiceException\n\tat com.johnsnowlabs.nlp.serialization.Feature.<init>(Feature.scala:36)\n\tat com.johnsnowlabs.nlp.serialization.MapFeature.<init>(Feature.scala:195)\n\tat com.johnsnowlabs.nlp.annotators.classifier.dl.DistilBertForQuestionAnswering.<init>(DistilBertForQuestionAnswering.scala:149)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonServiceException\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'distilbert-base-cased-distilled-squad'\n",
    "\n",
    "document_assembler = MultiDocumentAssembler() \\\n",
    "    .setInputCols([\"question\", \"context\", \"true_ans\"]) \\\n",
    "    .setOutputCols([\"document_question\", \"document_context\", \"document_ans\"])\n",
    "\n",
    "spanClassifier_loaded = DistilBertForQuestionAnswering.load(\"./{}_spark_nlp\".format(MODEL_NAME))\\\n",
    "  .setInputCols([\"document_question\",'document_context'])\\\n",
    "  .setOutputCol(\"answer\")\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    spanClassifier_loaded\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2925722",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/16 06:37:14 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-685b763d-8cdf-40b8-b985-0c8eac9ef807. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/05/16 06:37:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/05/16 06:37:15 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fa6965dfe20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"question\", StringType(), True),\n",
    "    StructField(\"context\", StringType(), True),\n",
    "    StructField(\"true_ans\", StringType(), True)\n",
    "])\n",
    "\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"pro2_13\") \\\n",
    "    .load()\n",
    "\n",
    "json_stream = kafka_stream.select(from_json(kafka_stream.value.cast(\"string\"), schema).alias(\"data\"))\n",
    "qa_stream = json_stream.select(\"data.question\", \"data.context\", \"data.true_ans\")\n",
    "\n",
    "query = qa_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .option(\"queryName\", \"QA_model\") \\\n",
    "    .trigger(processingTime = '5 seconds')\n",
    "    \n",
    "query.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6230b07c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n",
      "|question                                                        |context                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |true_ans                   |\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n",
      "|In what country is Normandy located?                            |The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.|France                     |\n",
      "|When were the Normans in Normandy?                              |The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.|10th and 11th centuries    |\n",
      "|From which countries did the Norse originate?                   |The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.|Denmark, Iceland and Norway|\n",
      "|Who was the Norse leader?                                       |The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.|Rollo                      |\n",
      "|What century did the Normans first gain their separate identity?|The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.|10th century               |\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.sql(\"SELECT * FROM QA_model\")\n",
    "df1.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad8d5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|            question|             context|            true_ans|              result|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|In what country i...|The Normans (Norm...|              France|            [France]|\n",
      "|When were the Nor...|The Normans (Norm...|10th and 11th cen...|[10th and 11th ce...|\n",
      "|From which countr...|The Normans (Norm...|Denmark, Iceland ...|[Denmark , Icelan...|\n",
      "|Who was the Norse...|The Normans (Norm...|               Rollo|             [Rollo]|\n",
      "|What century did ...|The Normans (Norm...|        10th century|              [10th]|\n",
      "|Who was the duke ...|The Norman dynast...|William the Conqu...|[William the Conq...|\n",
      "|Who ruled the duc...|The Norman dynast...|           Richard I|         [Richard I]|\n",
      "|What religion wer...|The Norman dynast...|            Catholic|          [Catholic]|\n",
      "|What is the origi...|The English name ...|              Viking| [Norseman , Viking]|\n",
      "|When was the Lati...|The English name ...|         9th century|       [9th century]|\n",
      "|When was the Duch...|In the course of ...|                 911|               [911]|\n",
      "|Who did Rollo sig...|In the course of ...|    King Charles III|[King Charles III...|\n",
      "|What river origin...|In the course of ...|               Seine|             [Seine]|\n",
      "|Who upon arriving...|Before Rollo's ar...|               Rollo|            [Viking]|\n",
      "|What was the Norm...|The descendants o...|         Catholicism|       [Catholicism]|\n",
      "|What part of Fran...|The descendants o...|               north|             [north]|\n",
      "|What was one of t...|The Normans there...|   fighting horsemen| [fighting horsemen]|\n",
      "|Who was the Norma...|Soon after the No...|        Seljuk Turks|[Pechenegs , the ...|\n",
      "|When did Herve se...|One of the first ...|               1050s|             [1050s]|\n",
      "|When did Robert C...|One of the first ...|               1060s|             [1060s]|\n",
      "|Who ruined Rousse...|One of the first ...|    Alexius Komnenos|  [Alexius Komnenos]|\n",
      "|What was the name...|Some Normans join...|             Afranji|           [Afranji]|\n",
      "|Who was the leade...|Some Normans join...|              Oursel|            [Oursel]|\n",
      "|Who did the Norma...|Some Normans join...|      Turkish forces|    [Armenian state]|\n",
      "|What were the ori...|Several families ...|    Norman mercenary|[The Raoulii were...|\n",
      "|What was the name...|Robert Guiscard, ...|     Robert Guiscard|   [Robert Guiscard]|\n",
      "|When did Dyrrachi...|Robert Guiscard, ...|                1082|     [February 1082]|\n",
      "|How many men were...|Robert Guiscard, ...|              30,000|          [30 , 000]|\n",
      "|Where did the Nor...|A few years after...|            Deabolis|          [Deabolis]|\n",
      "|Who was Robert's ...|A few years after...|            Bohemond|          [Bohemond]|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = pipeline.fit(df1).transform(df1)\n",
    "df2 = result.select(\"question\", \"context\", \"true_ans\", \"answer.result\")\n",
    "df2.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee279287",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()\n",
    "spark.sql(\"drop view QA_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "sp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
